import requests
from bs4 import BeautifulSoup
url='https://y20india.in/literacy-rates-in-india/'

r=requests.get(url)
htmlContent=r.content
#print(htmlContent)

soup = BeautifulSoup(htmlContent, 'html.parser')
#print(soup.prettify)

title=soup.title 
#print(type(title.string))

paras=soup.find_all('p')
#print(paras)

anchor=soup.find_all('a')
#print(anchor)

print(soup.find('p')['key'])



import requests
from bs4 import BeautifulSoup
import psycopg2
from psycopg2 import sql

# URL of the website with the table
url = 'https://y20india.in/literacy-rates-in-india/'

def scrape_table(url):
    # Send a GET request to the URL
    response = requests.get(url)
    if response.status_code != 200:
        raise Exception(f"Failed to load page {url}")

    # Parse the HTML content using BeautifulSoup
    soup = BeautifulSoup(response.text, 'html.parser')

    # Find the table element
    table = soup.find('table')
    if not table:
        raise Exception("No table found on the page")

    # Extract table headers
    headers = [th.text.strip() for th in table.find('tr').find_all('th')]

    # Extract table rows
    rows = []
    for tr in table.find_all('tr')[1:]:  # Skip the header row
        cells = [td.text.strip() for td in tr.find_all('td')]
        rows.append(cells)

    return headers, rows

def store_data_in_postgresql(headers, rows):
    # Database connection parameters
    conn = psycopg2.connect(
        dbname='your_database',
        user='your_username',
        password='your_password',
        host='127.0.0.1',
        port='5432'
    )
    cursor = conn.cursor()

    # Create table SQL query
    create_table_query = sql.SQL('''
        CREATE TABLE IF NOT EXISTS scraped_data (
            {}
        )
    ''').format(
        sql.SQL(', ').join(
            sql.Identifier(header) + sql.SQL(' TEXT') for header in headers
        )
    )
    cursor.execute(create_table_query)
    conn.commit()

    # Insert data SQL query
    insert_query = sql.SQL('''
        INSERT INTO scraped_data ({})
        VALUES ({})
    ''').format(
        sql.SQL(', ').join(map(sql.Identifier, headers)),
        sql.SQL(', ').join(sql.Placeholder() * len(headers))
    )

    for row in rows:
        cursor.execute(insert_query, row)

    conn.commit()
    cursor.close()
    conn.close()

if __name__ == '__main__':
    headers, rows = scrape_table(url)
    store_data_in_postgresql(headers, rows)





import requests
from bs4 import BeautifulSoup
url='https://y20india.in/literacy-rates-in-india/'

r=requests.get(url)
htmlContent=r.content
#print(r)

soup = BeautifulSoup(htmlContent, 'html.parser')
table=soup.find("table")
#print(table)
headers=table.find_all("th")
#print(headers)
titles=[]

for i in headers:
    title=i.text
    titles.append(title)

print(titles)


# import psycopg2
# from psycopg2 import sql

# # Database connection details (modify these with your credentials)
# hostname = "localhost"
# database = "scraping_db"
# username = "postgres"
# pwd = "password"
# port_id = 5432

# def connect():
#     """
#     Connects to the PostgreSQL database.
#     Returns a psycopg2 connection object or None on failure.
#     """
#     try:
#         connection = psycopg2.connect(
#             host=hostname,
#             dbname=database,
#             user=username,
#             password=pwd,
#             port=port_id
#         )
#         return connection
#     except Exception as error:
#         print("Error connecting to database:", error)
#         return None

# def create_cursor(connection):
#     """
#     Creates a cursor object from the provided connection.
#     Returns a psycopg2 cursor object or None on failure.
#     """
#     if connection:
#         try:
#             return connection.cursor()
#         except Exception as error:
#             print("Error creating cursor:", error)
#             return None
#     else:
#         print("Connection not established. Cannot create cursor.")
#         return None

# def close_connection(connection):
#     """
#     Closes the database connection if it's open.
#     """
#     if connection:
#         try:
#             connection.close()
#         except Exception as error:
#             print("Error closing database connection:", error)

# def insert(data):
#     """
#     Inserts a new record into the 'scraped_data' table.
#     Checks for duplicate Index_Name before insertion.

#     Args:
#         data (list): A list containing values to be inserted in the same
#                      order as the table columns.
#     """
#     connection = connect()
#     cursor = create_cursor(connection)

#     if connection and cursor:
#         try:
#             # Check if record exists (prevents duplicate Index_Name)
#             check_query = 'SELECT * FROM scraped_data WHERE "State/UT" = %s'
#             cursor.execute(check_query, (data[1],))
#             existing_record = cursor.fetchone()

#             if not existing_record:
#                 # Insert data if record doesn't exist
#                 insert_data = '''INSERT INTO scraped_data 
#                     ("State/UT", "Census 2011 Average", "Census 2011 Male" , "Census 2011 Female",
#                                   "NSO Survey 2017 Average" , "NSO Survey 2017 Male" , "NSO Survey 2017 Female"  
#                             )
#                     VALUES (%s, %s, %s, %s, %s, %s, %s)'''
#                 insert_values = data
#                 cursor.execute(insert_data, insert_values)
#                 connection.commit()
#                 print("Data added successfully")
#             else:
#                 print(f"Record with State '{data[1]}' already exists. Skipping insertion.")

#         except Exception as error:
#             print("Got an Error:", error)
#         finally:
#             close_connection(connection)

# def select_all():
#     """
#     Fetches all data from the 'scraped_data' table.
#     Prints retrieved data or a message if no data is found.
#     """
#     connection = connect()
#     cursor = create_cursor(connection)

#     if connection and cursor:
#         try:
#             # Fetch all data from scraped_data table
#             select_query = "SELECT * FROM scraped_data"
#             cursor.execute(select_query)
#             rows = cursor.fetchall()

#             # Print retrieved data
#             if rows:
#                 for row in rows:
#                     print(row)
#             else:
#                 print("No data found in scraped_data table.")

#         except Exception as error:
#             print("Got an Error:", error)
#         finally:
#             close_connection(connection)

# def update(data):
#     """
#     Updates existing data based on the provided State.

#     Args:
#         data (list): A list containing values to be updated in the same
#                      order as the table columns. The first element should
#                      be the State of the record to update.
#     """
#     connection = connect()
#     cursor = create_cursor(connection)

#     if connection and cursor:
#         try:
#             # Update data based on existing State
#             update_query = '''UPDATE scraped_data
#                               SET "Census 2011 Average" = %s, "Census 2011 Male" = %s, "Census 2011 Female" = %s,
#                                   "NSO Survey 2017 Average" = %s, "NSO Survey 2017 Male" = %s, "NSO Survey 2017 Female" = %s 
#                               WHERE "State/UT" = %s'''
#             update_values = data[1:] + [data[0]]  # Extract the index and append it at the end for the WHERE clause
#             cursor.execute(update_query, update_values)
#             connection.commit()
#             print("Data updated successfully")

#         except Exception as error:
#             print("Got an Error:", error)
#         finally:
#             close_connection(connection)

# def delete(state):
#     """
#     Deletes a record from the 'scraped_data' table based on the provided State.

#     Args:
#         state (str): The State value of the record to delete.
#     """
#     connection = connect()
#     cursor = create_cursor(connection)

#     if connection and cursor:
#         try:
#             # Delete data based on State
#             delete_data = 'DELETE FROM scraped_data WHERE "State/UT" = %s'
#             cursor.execute(delete_data, (state,))
#             connection.commit()
#             print("Data deleted successfully")

#         except Exception as error:
#             print("Got an Error:", error)
#         finally:
#             close_connection(connection)

# # Example data (replace with your actual data)
# new_data = ["HELLO", "120", "20", "130", "110", "125", "115"]
# insert(new_data)

# select_all()

# # Example data (replace with actual values)
# # updated_data = ["India", "135", "25", "140", "122", "130", "120"]
# # update(updated_data)
# # select_all()

# # delete("India")
# # select_all()



#select_all()

#delete("India")
# select_all()



















# def create_record(headers, values):
#     try:
#         con = psycopg2.connect(
#             dbname='scraping_db',
#             user='postgres',
#             password='password',
#             host='127.0.0.1',
#             port='5432'
#         )
#         cursor = con.cursor()

#         insert_query = sql.SQL('''
#             INSERT INTO scraped_data ({})
#             VALUES ({})
#         ''').format(
#             sql.SQL(', ').join(map(sql.Identifier, headers)),
#             sql.SQL(', ').join(sql.Placeholder() * len(headers))
#         )

#         cursor.execute(insert_query, values)
#         con.commit()
#     except Exception as e:
#         con.rollback()
#         raise Exception(f"Failed to create record: {e}")
#     finally:
#         cursor.close()
#         con.close()

# def read_records():
#     try:
#         con = psycopg2.conect(
#             dbname='scraping_db',
#             user='postgres',
#             password='password',
#             host='127.0.0.1',
#             port='5432'
#         )
#         cursor = con.cursor()

#         cursor.execute('SELECT * FROM scraped_data')
#         rows = cursor.fetchall()
#         con.commit()
#     except Exception as e:
#         raise Exception(f"Failed to read records: {e}")
#     finally:
#         cursor.close()
#         con.close()

#     return rows

# def update_record(condition, update_values):
#     try:
#         con = psycopg2.connect(
#             dbname='scraping_db',
#             user='postgres',
#             password='password',
#             host='127.0.0.1',
#             port='5432'
#         )
#         cursor = con.cursor()

#         set_clause = sql.SQL(', ').join(
#             sql.Composed([sql.Identifier(col), sql.SQL(' = '), sql.Placeholder()]) for col in update_values.keys()
#         )
#         where_clause = sql.SQL(' AND ').join(
#             sql.Composed([sql.Identifier(col), sql.SQL(' = '), sql.Placeholder()]) for col in condition.keys()
#         )

#         update_query = sql.SQL('''
#             UPDATE scraped_data
#             SET {}
#             WHERE {}
#         ''').format(set_clause, where_clause)

#         cursor.execute(update_query, list(update_values.values()) + list(condition.values()))
#         con.commit()
#     except Exception as e:
#         con.rollback()
#         raise Exception(f"Failed to update record: {e}")
#     finally:
#         cursor.close()
#         con.close()

# def delete_record(condition):
#     try:
#         con = psycopg2.connect(
#             dbname='scraping_db',
#             user='postgres',
#             password='password',
#             host='127.0.0.1',
#             port='5432'
#         )
#         cursor = con.cursor()

#         where_clause = sql.SQL(' AND ').join(
#             sql.Composed([sql.Identifier(col), sql.SQL(' = '), sql.Placeholder()]) for col in condition.keys()
#         )

#         delete_query = sql.SQL('''
#             DELETE FROM scraped_data
#             WHERE {}
#         ''').format(where_clause)

#         cursor.execute(delete_query, list(condition.values()))
#         con.commit()
#     except Exception as e:
#         con.rollback()
#         raise Exception(f"Failed to delete record: {e}")
#     finally:
#         cursor.close()
#         con.close()
